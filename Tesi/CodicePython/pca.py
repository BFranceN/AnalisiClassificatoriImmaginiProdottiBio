# -*- coding: utf-8 -*-
"""PCA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11WT2eCB_0A4QCtbHPhaRxyTFoQ4e8zxw
"""

#Import Libraries

import numpy as np #Numerical Computing.
import os #I/O.
import tensorflow as tf #Machine Learning.
from tensorflow.keras.preprocessing import image_dataset_from_directory #Dataset Generator.
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from skimage.transform import resize
from skimage.io import imread
from sklearn.metrics import classification_report,accuracy_score,confusion_matrix
import matplotlib.pyplot as plt
from sklearn import svm
import pandas as pd
import pickle
from sklearn import metrics
import cv2 
import seaborn as sns


from skimage.color import rgb2gray
from skimage.filters import sobel
from skimage.filters.rank import entropy
from skimage.morphology import disk
import random
from sklearn.preprocessing import StandardScaler
from sklearn import decomposition
tf.test.gpu_device_name()

from google.colab import drive
drive.mount('/content/drive')

!unzip "../content/drive/MyDrive/Bio/Data.zip" -d "/content"

# A questo punto uso la PCA per trainare il modello

Categories = ['B','N']
flat_data_arr = []
target_arr = []


bio_file = []
no_bio_file = []

#test 
tb = []
tnb = []

#per rendere gli esperimenti riproducibili
random.seed(77)

# A questo punto uso la PCA per trainare il modello

Categories = ['B','N']
flat_data_arr = []
target_arr = []


bio_file = []
no_bio_file = []

#test 
tb = []
tnb = []

#per rendere gli esperimenti riproducibili
random.seed(77)


#Concateno tutte le immagini

for indice in range(1,5):
  pathb = '../content/TRAIN.'+str(indice)+'/B'
  pathnb = '../content/TRAIN.'+str(indice)+'/N'

  bio_file +=  [pathb+'/'+fn for fn in os.listdir('../content/TRAIN.'+str(indice)+'/B') if ("ORI" in fn) ]
  no_bio_file +=  [pathnb+'/'+fn for fn in os.listdir('../content/TRAIN.'+str(indice)+'/N') if ("ORI" in fn)]


#metto alla pari le due classi
fn_bio = random.sample(bio_file,6000 )
fn_nobio = random.sample(no_bio_file,6000 )

#Abbiamo ora un dataset perfettamente bilanciato
print(f"bio -> {len(fn_bio)}")
print(f"no_bio -> { len(fn_nobio)}")

#Ora che ho i filename posso leggere le varie immaginidata

img = cv2.cvtColor(cv2.imread(fn_bio[10]), cv2.COLOR_BGR2RGB)
plt.imshow(img)
plt.show()

print(img.shape)

red,green,blue =  cv2.split(img)


plt.imshow(red)
plt.show()

plt.imshow(green)
plt.show()

plt.imshow(blue)
plt.show()

#PCA 3 componenti
from matplotlib.colors import ListedColormap
from matplotlib import pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import seaborn as sns

pca = decomposition.PCA()
pca.n_components = 3
pca_data = pca.fit_transform(flat_data).T
print(pca_data.shape)
pca_new = np.vstack((pca_data, target)).T
print(pca_new.shape)
df = pd.DataFrame(data=pca_new, columns = ["1st","2nd",'3th',"labels"])
print(df)












'''
fig = plt.figure(figsize=(10, 6))
ax = Axes3D(fig)
fig.add_axes(ax)

# find all the unique labels in the 'name' column
labels = np.unique(df['labels'])
# get palette from seaborn
palette = sns.color_palette("tab10", len(labels))

# plot
for label, color in zip(labels, palette):
    df1 = df[df['labels'] == label]
    ax.scatter(df1['1st'], df1['2nd'], df1['3th'],
               s=40, marker='x', color=color, alpha=0.8, label=label)
ax.set_xlabel('1st')
ax.set_ylabel('2nd')
ax.set_zlabel('3th')

# legend
plt.legend(bbox_to_anchor=(1.05, 1), loc=2)
plt.show()
'''


#bio sono verdi 
#non bio rossi
colors = ListedColormap(['g','r'])

labels = df['labels'].to_numpy(dtype='int')



ax = plt.figure(figsize=(16,12)).gca(projection='3d')
ax.scatter(
    xs=df['1st'], 
    ys=df['2nd'], 
    zs=df['3th'], 
    c=df["labels"],
    label = labels,
    cmap= colors,s=50
    
)
ax.set_xlabel('pca-one')
ax.set_ylabel('pca-two')
ax.set_zlabel('pca-three')


plt.show()