# -*- coding: utf-8 -*-
"""TestFiltroUniversale.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AS_7KQYH4EOyf6I3kYerZcm5BeKsgEhK
"""

#Import Libraries

import numpy as np #Numerical Computing.
import os #I/O.
import tensorflow as tf #Machine Learning.
from tensorflow.keras.preprocessing import image_dataset_from_directory #Dataset Generator.
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from skimage.transform import resize
from skimage.io import imread
from sklearn.metrics import classification_report,accuracy_score,confusion_matrix
import matplotlib.pyplot as plt
from sklearn import svm
import pandas as pd
import pickle
from sklearn import metrics
import cv2 

from skimage.color import rgb2gray
from skimage.filters import sobel
from skimage.filters.rank import entropy
from skimage.morphology import disk
tf.test.gpu_device_name()

from google.colab import drive
drive.mount('/content/drive')

!unzip "../content/drive/MyDrive/Bio/Data.zip" -d "/content"

Categories = ['B','N']
flat_data_arr = []
target_arr = []




#setto i percorsi

pathB = '../content/TRAIN.1/B'


pathN ='../content/TRAIN.1/N'

listPath = [pathB, pathN]


#scelgo quale filtro usare
filtro = int(input("Scegli quale filtro usare:"))



#LOAD BIO
for i in range (0,2):
  conteggio = 0
  path = listPath[i]
  print(path)
  for img in os.listdir(path):
    if(conteggio == 6000):break
    print(conteggio)

    img_array = imread(os.path.join(path,img), as_gray=True) #immagine

    if(filtro == -1):
      img = resize(img_array, (200,200))
      print(f"max img -> {img.max()}")
      flat_data_arr.append(img.flatten())



    if(filtro == 0):
       img_array = np.float32(img_array) # per fare la dft
       dft = cv2.dft(img_array, flags = cv2.DFT_COMPLEX_OUTPUT)
       dft_shift = np.fft.fftshift(dft) #center to the centre
       img_spectrum = 20*np.log(cv2.magnitude(dft_shift[:,:,0], dft_shift[:,:,1]) + 1)
       img_spectrum = resize(img_spectrum,(32,32))
       flat_data_arr.append(img_spectrum.flatten())
    if(filtro == 1):
      sobel_img = sobel(img_array)
      sobel_img = resize(sobel_img,(32,32))
      flat_data_arr.append(sobel_img.flatten())
      if conteggio == 10:
        plt.imshow(sobel_img, cmap ='Greys')
        plt.show()
      
    if (filtro == 2):
      entropy_img = entropy(img_array, disk(1))
      entropy_img = resize(entropy_img,(32,32)) / 255
      flat_data_arr.append(entropy_img.flatten())
      if conteggio==10:
        plt.imshow(entropy_img)
        plt.show()





    '''
    (r, g, b) = cv2.split(img_array)
    #Questa sezione è stata scritta per analizzare i 3 canali RGB 
    if(choose == 0):
      imgc = img_array
    elif(choose == 1):
      imgc = r
    elif (choose == 2):
      imgc = g
    else:
      imgc = b
    
    if(choose ==0):
      img_resized = resize(imgc,(32,32,3))
    else:
      img_resized = resize(imgc,(32,32))
    flat_data_arr.append(img_resized.flatten())
    ''' 
    target_arr.append(Categories.index(Categories[i]))
    conteggio = conteggio + 1
  print(f"sezione {i} caricata")

print(f"look -> {flat_data_arr[0].shape}")
print("entro nella zona1")
flat_data = np.array(flat_data_arr)
print("è la zona 2 a darmi problemi?")
target = np.array(target_arr)

#Creo un dataframe dai dati np
print("entro nella zona2")
df = pd.DataFrame(flat_data)
df['Target'] = target
print("fatto")

x=df.iloc[:,:-1] # qui prendo l'immagine (tranne la label)
y=df.iloc[:,-1] # qui prendo solo la label


print(np.unique(y))

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.50,random_state=77,stratify=y)

print(f" max -> {x.max()}")
print(y)
print(df['Target'])

scelta_modello = int(input("scegli il modello:"))

#SVM
if(scelta_modello == 0):
  #param_grid={'C':[10,0.1],'kernel':['linear','poly','rbf'], 'gamma':[0.1,1]} 
  #param_grid={'C':[1],'kernel':['poly'], 'gamma':[0.1]} 

  param_grid={'C':[0.1],'kernel':['rbf']}


 # param_grid={'kernel':['rbf','poly'], 'gamma':[1, 10,100]} 

  svc=svm.SVC(probability=True)
  print("inizio allenamento")
  model=GridSearchCV(svc,param_grid)  

  # test if CPU and GPU are visible
  c = tf.config.get_visible_devices()
  print(c)
  # [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),
  #  PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]  

  model.fit(x_train,y_train)
  print('Fine allenamento')
  print(model.best_params_)

#RANDOM FOREST
elif(scelta_modello == 1):
  #param_grid = {'bootstrap':[True], 'n_estimators':[300,400,500], 'max_depth':[50,70]}
  param_grid = {'bootstrap':[True], 'n_estimators':[300], 'max_depth':[70], ''}
  clf=RandomForestClassifier(n_estimators=100)
  print("inizio allenamento")
  model=GridSearchCV(clf, param_grid)
  model.fit(x_train,y_train)
  print("fine allenamento")
  print (model.best_params_)


#KNN
elif(scelta_modello == 2):
  knn = KNeighborsClassifier()
  k_range = list(range(3,15,2))
  param_grid = dict(n_neighbors = k_range, weights = ["distance"], metric=["manhattan"])
  grid = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy', return_train_score=True, verbose=2)
  #fitting the model for grid search
  grid_search = grid.fit(x_train, y_train)
  print(grid_search.best_params_)
  model = grid_search

model = grid_search

y_pred = model.predict(x_test)
print(f"predizione modello -> {y_pred}")
print(f"label reali -> {np.array(y_test)}")

#vediamo l'accuratezza:
print(f"The model is {accuracy_score(y_pred,y_test)*100}% accurate")
#Confusion matrix
confusion_matrix = metrics.confusion_matrix(y_test,y_pred)
cm_display = metrics.ConfusionMatrixDisplay (confusion_matrix = confusion_matrix, display_labels = ['B','N'])
cm_display.plot()
plt.show()
#Metrics
#of the positives predicted, what percentage is truly positive?
precision = metrics.precision_score(y_test, y_pred)
print(f"Precision: {precision*100}% ")
#how good the model is at predicting positives?
sensitivity_recall = metrics.recall_score(y_test, y_pred)
print(f"Sensitivity: {sensitivity_recall*100}% ")
#how good the model is at predicting negatives?
specificity = metrics.recall_score(y_test, y_pred, pos_label=0)
print(f"Specificity: {specificity*100}% ")
f1_score = metrics.f1_score(y_test, y_pred)
print(f"F-Score: {f1_score * 100}")